# **Preliminary Project Information**
**Student Name:** Fatimah Alali-g202423180

**Project title:** Graph-Guided Story Generation and Semantic Enrichment with Large Vision-Language Models

**Author names for refrence paper:** Fan Lu, Wei Wu, Kecheng Zheng, et al.

**Date:** September 25, 2024
## Problem Statement
The current paper ([Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning](https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_Benchmarking_Large_Vision-Language_Models_via_Directed_Scene_Graph_for_Comprehensive_CVPR_2025_paper.pdf))
emploes Large Vision-Language Models (LVLMs) that excel at generating comprehensive single-image captions. However, it lack in creation coherent, emotionally engaging multi-sentence stories that capture both visual details and narrative flow
The benchmark for the paper achieved a good result in  scene graph annotations for evaluating comprehensive image captioning. The gap found in the current benchmark are:
1.Temporal Narrative Structure: Existing models generate static descriptions rather than progressive storytelling
2.Emotional Context Integration: Current approaches focus on visual accuracy but lack emotional depth and contextual richness
3.Multi-Sentence Coherence: No systematic framework exists for maintaining narrative consistency across multiple sentences
4.Structured Evaluation: Limited metrics for assessing story quality beyond traditional captioning measures
